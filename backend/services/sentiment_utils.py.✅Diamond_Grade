"""
backend/services/sentiment_utils.py
----------------------------------

Production-ready Sentiment classifier for corporate announcements / news.

Features
- Pydantic-based SentimentSettings (env-friendly)
- SentimentClassifier class (dependency-injectable)
- Observability & audit hooks (duck-typed / Protocol)
- Async + sync LLM client support (Protocol)
- Compiled regex phrase matching for O(1)-like performance on phrase lookup
- Simple negation handling for nearby tokens
- Input validation and truncation (safe defaults)
- Returns a typed SentimentResult (Pydantic) with sources and raw responses
- Loads external keyword JSON files if provided, falls back to in-file defaults

Author: Assistant (generated)
Date: 29-Oct-2025
"""
from __future__ import annotations

import asyncio
import json
import logging
import os
import re
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Callable, Dict, List, Mapping, Optional, Pattern, Protocol, Tuple

from pydantic import BaseModel, Field, root_validator, validator
from pydantic_settings import SettingsConfigDict

# -------------------------
# Observability Protocols
# -------------------------
class ObservabilityClientProtocol(Protocol):
    """Lightweight protocol for observability hooks used by SentimentClassifier."""

    def inc_metric(self, name: str, value: int = 1) -> None:
        ...

    def audit(self, action: str, payload: Mapping[str, Any]) -> None:
        ...

    def debug(self, msg: str, **kw: Any) -> None:
        ...

# -------------------------
# LLM Client Protocol
# -------------------------
class LLMClientProtocol(Protocol):
    """
    Minimal LLM client protocol.

    Implementations should provide either:
    - async classify_async(text: str, **kw) -> dict
    OR
    - sync classify_sync(text: str, **kw) -> dict

    The classifier will detect which method is present.
    """
    # optional methods â€” not enforced at runtime, but helpful for type-checkers
    async def classify_async(self, text: str, **kwargs: Any) -> Mapping[str, Any]:
        ...

    def classify_sync(self, text: str, **kwargs: Any) -> Mapping[str, Any]:
        ...

# -------------------------
# Default Keywords (from user's list)
# -------------------------
DEFAULT_KEYWORDS: Dict[str, List[str]] = {
    "positive": [
        # Financial Performance
        "profit increase", "revenue up", "improved margins", "record earnings",
        "strong performance", "positive results", "dividend declared", "bonus issue",
        "profit before tax up", "EBITDA growth", "net profit rises", "better-than-expected results",
        # Corporate Actions
        "bonus shares", "stock split", "share buyback", "interim dividend", "final dividend",
        "special dividend", "rights issue", "merger approved", "acquisition completed",
        "strategic investment received",
        # Business Growth
        "order received", "contract won", "MoU signed", "expansion plan", "capacity addition",
        "new plant commissioned", "launch of new product", "partnership announced",
        "global collaboration", "approval received", "regulatory clearance obtained",
        "export order secured", "listing approval received", "new market entry", "government tender win",
        # Management / ESG
        "appointment of experienced CEO", "credit rating upgraded", "positive outlook assigned",
        "successful fund raise", "debt reduced", "zero debt company", "sustainability initiative launched",
        "awards", "recognition received",
    ],
    "negative": [
        # Financial Decline
        "loss reported", "revenue decline", "poor results", "weak quarter", "profit down",
        "negative EBITDA", "margin contraction", "cost increase", "impairment loss", "write-off",
        "higher expenses",
        # Legal / Compliance
        "SEBI investigation", "penalty imposed", "notice received", "tax demand", "legal proceedings",
        "non-compliance", "fraud detected", "resignation of auditor", "resignation of independent director",
        "resignation of CFO", "resignation of CEO", "suspension of trading", "delisting notice",
        "credit rating downgraded",
        # Operational / Strategic Setbacks
        "project delayed", "contract terminated", "order cancelled", "plant shutdown", "strike",
        "labour unrest", "disruption in production", "fire", "accident", "fatality", "loss of customer",
        "supply chain issue", "cyber incident", "data breach",
        # Market / Financial Stress
        "debt default", "loan repayment delay", "liquidity crunch", "bank account frozen",
        "insolvency", "bankruptcy", "NCLT filing", "negative cash flow", "promoter pledged shares",
    ],
    "neutral": [
        "board meeting scheduled", "results date announced", "record date fixed", "AGM notice",
        "EGM notice", "book closure", "clarification on news item", "trading window closure",
        "investor presentation uploaded", "press release issued", "outcome of meeting", "re-appointment",
        "change in company name", "alteration of memorandum", "registered office change",
        "compliance certificate", "shareholding pattern filed", "change in auditor",
        "change in company secretary", "alteration in authorised capital", "revision in policies",
        "adoption of new accounting standard",
    ],
    "ambiguous": [
        "merger", "acquisition", "rights issue", "preferential allotment", "board approved",
        "strategic review", "promoter transaction", "stake sale", "stake acquisition",
        "change in shareholding", "restructuring plan", "management change",
        "related party transaction", "joint venture", "MoU", "debt restructuring",
        "capital infusion", "fund raise",
    ],
}

# -------------------------
# Settings
# -------------------------
class SentimentSettings(BaseModel):
    """
    Environment-configurable settings for SentimentClassifier.

    Use environment variables with prefix SENTIMENT_, e.g.:
      SENTIMENT_LLM_WEIGHT=0.4
      SENTIMENT_KEYWORD_WEIGHT=0.6
      SENTIMENT_POS_THRESHOLD=0.66
      SENTIMENT_NEG_THRESHOLD=0.34
    """
    model_config = SettingsConfigDict(env_prefix="SENTIMENT_", env_file=".env", env_file_encoding="utf-8")

    LLM_WEIGHT: float = Field(0.4, ge=0.0, le=1.0)
    KEYWORD_WEIGHT: float = Field(0.6, ge=0.0, le=1.0)
    POS_THRESHOLD: float = Field(0.66, ge=0.0, le=1.0)
    NEG_THRESHOLD: float = Field(0.34, ge=0.0, le=1.0)

    # Behavior
    MAX_CHARS: int = Field(100_000, ge=1024)
    AMBIGUOUS_TOLERANCE: float = Field(0.15, ge=0.0, le=1.0)  # how close to neutral to allow 'Ambiguous'
    NEGATION_WINDOW: int = Field(3, ge=1, le=10)  # words before keyword that negate it
    KEYWORDS_DIR: Optional[str] = None  # optional directory to load JSON keyword lists

    @root_validator(pre=True)
    def normalize_weights(cls, values):
        """Ensure LLM_WEIGHT + KEYWORD_WEIGHT == 1.0 (normalize proportionally)."""
        lw = float(values.get("LLM_WEIGHT", 0.4))
        kw = float(values.get("KEYWORD_WEIGHT", 0.6))
        total = lw + kw
        if total <= 0:
            values["LLM_WEIGHT"] = 0.4
            values["KEYWORD_WEIGHT"] = 0.6
        else:
            values["LLM_WEIGHT"] = lw / total
            values["KEYWORD_WEIGHT"] = kw / total
        return values

# -------------------------
# Result model
# -------------------------
class SentimentSource(BaseModel):
    source: str
    label: str
    score: float
    matches: Optional[Dict[str, List[str]]] = None
    raw_counts: Optional[Dict[str, int]] = None


class SentimentResult(BaseModel):
    label: str
    score: float
    emoji: str
    sources: List[SentimentSource]
    raw_responses: Dict[str, Any] = Field(default_factory=dict)

# -------------------------
# Utilities
# -------------------------
_LOG = logging.getLogger("backend.services.sentiment_utils")


def _safe_str(x: Any) -> str:
    try:
        return str(x)
    except Exception:
        return "<unserializable>"


def _compile_phrase_regex(phrases: List[str]) -> Optional[Pattern[str]]:
    """
    Build a compiled regex that matches any of the phrases using word boundaries.
    Sort by length descending to prefer longer phrases first (avoids partial matches).
    Returns None if phrases empty.
    """
    if not phrases:
        return None
    # Escape phrases and ensure word boundaries; include hyphens and slashes inside word char class
    escaped = [re.escape(p.strip()) for p in sorted(set(phrases), key=lambda s: -len(s))]
    # Use word boundary \b; allow internal punctuation via lookarounds is complex, but this is pragmatic.
    pattern = r"\b(?:" + "|".join(escaped) + r")\b"
    try:
        return re.compile(pattern, re.IGNORECASE)
    except Exception:
        return re.compile("|".join(escaped), re.IGNORECASE)


def _tokenize(text: str) -> List[str]:
    """Simple tokenization: lowercase words and hyphenated tokens."""
    return re.findall(r"[a-z0-9\-']+", text.lower()) if text else []


def _detect_negations(tokens: List[str], index: int, window: int) -> bool:
    """
    Detect common negation words within `window` tokens before the token at `index`.
    Returns True if a negation is present near the token.
    """
    if index < 0:
        return False
    start = max(0, index - window)
    scope = tokens[start:index]
    for w in ("no", "not", "without", "never", "nil", "none", "unless"):
        if w in scope:
            return True
    return False

# -------------------------
# Classifier Implementation
# -------------------------
class SentimentClassifier:
    """
    Main sentiment classifier.

    Example:
        settings = SentimentSettings()
        classifier = SentimentClassifier(settings=settings, observability=obs, llm_client=llm)
        result = await classifier.compute_sentiment_with_llm_async(text)
        # or
        result = classifier.compute_sentiment(text)
    """

    def __init__(
        self,
        settings: Optional[SentimentSettings] = None,
        keywords_dir: Optional[str] = None,
        observability: Optional[ObservabilityClientProtocol] = None,
        llm_client: Optional[LLMClientProtocol] = None,
        fallback_keywords: Optional[Mapping[str, List[str]]] = None,
    ):
        self.settings = settings or SentimentSettings()
        # deterministic order: explicit keywords_dir > settings.KEYWORDS_DIR
        effective_dir = keywords_dir or self.settings.KEYWORDS_DIR
        self.keywords = self._load_keywords(effective_dir, fallback_keywords or DEFAULT_KEYWORDS)
        # Pre-compile regex patterns per category for speed
        self._regex = {
            cat: _compile_phrase_regex(phrases)
            for cat, phrases in self.keywords.items()
        }
        self.observability = observability
        self.llm_client = llm_client

    # -------------------------
    # Keyword loading
    # -------------------------
    def _load_keywords(self, keywords_dir: Optional[str], fallback: Mapping[str, List[str]]) -> Dict[str, List[str]]:
        """
        Load keyword lists from JSON files in `keywords_dir` if present.
        Expected file names: positive.json, negative.json, neutral.json, ambiguous.json
        Format: {"positive": [...], ...} or each file contains a list.
        Falls back to `fallback` if loading fails.
        """
        if not keywords_dir:
            return {k: list(v) for k, v in fallback.items()}
        try:
            p = Path(keywords_dir)
            if not p.exists() or not p.is_dir():
                _LOG.debug("keywords_dir '%s' not found or not dir; using fallback", keywords_dir)
                return {k: list(v) for k, v in fallback.items()}
            out: Dict[str, List[str]] = {}
            # try combined json
            combined = p / "sentiment_keywords.json"
            if combined.exists():
                loaded = json.loads(combined.read_text(encoding="utf-8"))
                for cat in ("positive", "negative", "neutral", "ambiguous"):
                    out[cat] = [s for s in (loaded.get(cat) or []) if isinstance(s, str)]
                return out
            # otherwise per-category files
            for cat in ("positive", "negative", "neutral", "ambiguous"):
                f = p / f"{cat}.json"
                if f.exists():
                    arr = json.loads(f.read_text(encoding="utf-8"))
                    out[cat] = [s for s in arr if isinstance(s, str)]
                else:
                    out[cat] = list(fallback.get(cat, []))
            return out
        except Exception as e:
            _LOG.exception("Failed loading keywords from %s: %s", keywords_dir, e)
            return {k: list(v) for k, v in fallback.items()}

    # -------------------------
    # Core keyword scoring
    # -------------------------
    def _find_phrase_matches(self, text: str, cat: str) -> List[str]:
        """
        Return list of matched phrases for category `cat` using compiled regex.
        Falls back to scanning if regex is None.
        """
        regex = self._regex.get(cat)
        if not text:
            return []
        matches: List[str] = []
        if regex:
            for m in regex.finditer(text):
                matches.append(m.group(0).strip())
            # remove duplicates preserving order
            seen = set()
            out = []
            for s in matches:
                key = s.lower()
                if key not in seen:
                    seen.add(key)
                    out.append(s)
            return out
        # fallback naive search (should be rare)
        for p in self.keywords.get(cat, []):
            if p.lower() in text.lower():
                matches.append(p)
        return matches

    def _keyword_counts(self, text: str) -> Dict[str, Any]:
        """
        Compute phrase matches and counts for each category.
        Also apply a simple negation heuristic to invert single-word matches if needed.
        Returns a dict with counts, matches, raw_counts.
        """
        if not text:
            return {
                "pos": 0, "neg": 0, "neu": 0, "amb": 0,
                "matches": {"positive": [], "negative": [], "neutral": [], "ambiguous": []},
                "raw_counts": {"pos": 0, "neg": 0, "neu": 0, "amb": 0},
            }
        text_l = text
        tokens = _tokenize(text_l)
        matches = {"positive": [], "negative": [], "neutral": [], "ambiguous": []}

        for cat_key, cat_name in (("positive", "positive"), ("negative", "negative"), ("neutral", "neutral"), ("ambiguous", "ambiguous")):
            found = self._find_phrase_matches(text_l, cat_key)
            # also add token-level matches that weren't captured as phrases
            # ensure uniqueness
            seen = set(s.lower() for s in found)
            for f in found:
                matches[cat_key].append(f)
            # token-level fallback
            for i, t in enumerate(tokens):
                if t in (kw.lower() for kw in self.keywords.get(cat_key, [])) and t not in seen:
                    # detect negation in window
                    negated = _detect_negations(tokens, i, self.settings.NEGATION_WINDOW)
                    if negated:
                        # if negated, invert: e.g., "no profit" should not add 'positive'
                        # For ambiguous words, ignore negation inversion (context dependent)
                        continue
                    matches[cat_key].append(t)
                    seen.add(t)

        raw_counts = {
            "pos": len(matches["positive"]),
            "neg": len(matches["negative"]),
            "neu": len(matches["neutral"]),
            "amb": len(matches["ambiguous"]),
        }
        return {
            "pos": raw_counts["pos"],
            "neg": raw_counts["neg"],
            "neu": raw_counts["neu"],
            "amb": raw_counts["amb"],
            "matches": matches,
            "raw_counts": raw_counts,
        }

    def _keyword_score_from_counts(self, pos: int, neg: int, neu: int, amb: int) -> float:
        """
        Convert raw counts into a 0..1 score where 1=very positive, 0=very negative.
        Ambiguous counts count as half-positive (0.5 contribution).
        """
        denom = max(1, pos + neg + neu + amb)
        raw = (pos - neg + 0.5 * amb) / denom
        raw = max(-1.0, min(1.0, raw))
        score = (raw + 1.0) / 2.0
        return float(max(0.0, min(1.0, score)))

    def _label_from_score(self, score: float) -> str:
        """
        Map blended score (0..1) to label using thresholds.
        """
        if score >= self.settings.POS_THRESHOLD:
            return "Positive"
        if score <= self.settings.NEG_THRESHOLD:
            return "Negative"
        return "Neutral"

    def _emoji_for_label(self, label: str) -> str:
        return {"Positive": "ðŸŸ¢", "Negative": "ðŸ”´", "Neutral": "ðŸŸ§", "Ambiguous": "ðŸŸ¦"}.get(label, "ðŸŸ§")

    # -------------------------
    # Public: keyword-only sentiment
    # -------------------------
    def compute_sentiment(self, main_text: str, llm_raw: Optional[Mapping[str, Any]] = None) -> SentimentResult:
        """
        Compute sentiment deterministically using keywords and optional LLM raw output.
        Returns SentimentResult (Pydantic).
        """
        if not isinstance(main_text, str):
            raise ValueError("main_text must be a string")
        if len(main_text) > self.settings.MAX_CHARS:
            main_text = main_text[: self.settings.MAX_CHARS]

        kw = self._keyword_counts(main_text)
        keyword_score = self._keyword_score_from_counts(kw["pos"], kw["neg"], kw["neu"], kw["amb"])
        ambiguous_found = kw["amb"] > 0

        # Normalize/validate llm_raw defensively
        llm_score: Optional[float] = None
        llm_label: Optional[str] = None
        if llm_raw and isinstance(llm_raw, Mapping):
            # accept different keys
            v = None
            for key in ("score", "confidence", "probability"):
                if key in llm_raw:
                    v = llm_raw.get(key)
                    break
            if v is not None:
                try:
                    llm_score = float(v)
                except Exception:
                    llm_score = None
            llm_label = llm_raw.get("label") or llm_raw.get("sentiment") if isinstance(llm_raw, Mapping) else None

        # default neutral llm score if missing so keywords matter
        if llm_score is None:
            llm_score = 0.5

        final_score = float(self.settings.LLM_WEIGHT * llm_score + self.settings.KEYWORD_WEIGHT * keyword_score)

        # Ambiguous logic: only mark Ambiguous if ambiguous words present AND
        # final_score is within AMBIGUOUS_TOLERANCE of neutral (0.5)
        if ambiguous_found and abs(final_score - 0.5) <= self.settings.AMBIGUOUS_TOLERANCE:
            label = "Ambiguous"
        else:
            label = self._label_from_score(final_score)

        emoji = self._emoji_for_label(label)

        sources: List[SentimentSource] = []
        # LLM source (if provided)
        if llm_raw is not None:
            try:
                sources.append(
                    SentimentSource(
                        source="llm",
                        label=str(llm_label or ("Positive" if llm_score > 0.6 else "Negative" if llm_score < 0.4 else "Neutral")),
                        score=round(float(llm_score), 3),
                        matches=None,
                        raw_counts=None,
                    )
                )
            except Exception:
                # defensive
                sources.append(SentimentSource(source="llm", label="Neutral", score=round(float(llm_score), 3)))

        # Keyword source
        sources.append(
            SentimentSource(
                source="keyword",
                label=("Positive" if keyword_score > 0.6 else "Negative" if keyword_score < 0.4 else "Neutral"),
                score=round(keyword_score, 3),
                matches=kw.get("matches"),
                raw_counts=kw.get("raw_counts"),
            )
        )

        # Observability hooks (best-effort)
        try:
            if self.observability:
                self.observability.inc_metric("sentiment.computed", 1)
                if llm_raw is not None:
                    self.observability.inc_metric("sentiment.llm_used", 1)
                self.observability.audit("sentiment.computed", {"label": label, "score": final_score})
        except Exception:
            _LOG.debug("Observability hook failed (ignored)")

        return SentimentResult(
            label=label,
            score=round(final_score, 3),
            emoji=emoji,
            sources=sources,
            raw_responses={"llm": dict(llm_raw) if llm_raw is not None else None, "keyword": {"matches": kw.get("matches"), "raw_counts": kw.get("raw_counts")}},
        )

    # -------------------------
    # Async wrapper: call LLM and combine
    # -------------------------
    async def compute_sentiment_with_llm_async(self, main_text: str, timeout: float = 10.0) -> SentimentResult:
        """
        Async method: if llm_client is present, call it (async or via threadpool),
        then call compute_sentiment to blend results.

        Timeout is enforced for both async LLMs and sync LLMs run in executor.
        """
        if not isinstance(main_text, str):
            raise ValueError("main_text must be a string")
        if len(main_text) > self.settings.MAX_CHARS:
            main_text = main_text[: self.settings.MAX_CHARS]

        llm_raw: Optional[Mapping[str, Any]] = None
        if self.llm_client:
            # prefer async method if present
            try:
                classify_async = getattr(self.llm_client, "classify_async", None)
                classify_sync = getattr(self.llm_client, "classify_sync", None)
                if callable(classify_async):
                    # await with timeout
                    llm_raw = await asyncio.wait_for(classify_async(main_text), timeout=timeout)
                elif callable(classify_sync):
                    loop = asyncio.get_running_loop()
                    llm_raw = await asyncio.wait_for(loop.run_in_executor(None, classify_sync, main_text), timeout=timeout)
                else:
                    _LOG.debug("llm_client has neither classify_async nor classify_sync; skipping LLM")
            except asyncio.TimeoutError:
                _LOG.warning("LLM classification timed out (timeout=%s)", timeout)
                llm_raw = {"error": "timeout"}
            except Exception as e:
                _LOG.exception("LLM classification failed: %s", e)
                llm_raw = {"error": _safe_str(e)}

        # final blend
        return self.compute_sentiment(main_text, llm_raw=llm_raw)

# -------------------------
# Convenience module-level classifier (singleton-like)
# -------------------------
_default_classifier: Optional[SentimentClassifier] = None


def get_default_classifier(**kw: Any) -> SentimentClassifier:
    global _default_classifier
    if _default_classifier is None:
        settings = SentimentSettings()
        # allow passing keywords_dir or observability via kw
        _default_classifier = SentimentClassifier(settings=settings, **kw)
    return _default_classifier


# -------------------------
# Module-level helpers for backward compatibility
# -------------------------
def compute_sentiment(main_text: str, llm_raw: Optional[Mapping[str, Any]] = None) -> Dict[str, Any]:
    """
    Backwards-compatible function returning dict (non-Pydantic).
    """
    c = get_default_classifier()
    res = c.compute_sentiment(main_text, llm_raw)
    return res.dict()


async def compute_sentiment_with_llm_async(main_text: str, timeout: float = 10.0) -> Dict[str, Any]:
    c = get_default_classifier()
    res = await c.compute_sentiment_with_llm_async(main_text, timeout=timeout)
    return res.dict()


# -------------------------
# Quick manual smoke test when run directly
# -------------------------
if __name__ == "__main__":
    # Simple smoke tests
    import asyncio as _asyncio

    cls = get_default_classifier()
    samples = [
        "Company reported profit and revenue growth and approved a dividend.",
        "Company detected fraud and faces regulatory fines; earnings warn investors.",
        "Board meeting notice: AGM scheduled and shareholding pattern updated.",
        "Company announces merger and large expansion; market reaction unclear.",
    ]

    async def _run():
        for s in samples:
            r = await cls.compute_sentiment_with_llm_async(s, timeout=5.0)
            print("INPUT:", s)
            print("=>", r.json(indent=2))
            print("-" * 60)

    _asyncio.run(_run())